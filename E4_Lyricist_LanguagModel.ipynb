{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "digital-story",
   "metadata": {},
   "source": [
    "#  LMS Exploration | Lyricist Language Model\n",
    "***\n",
    "\n",
    "**[Introduce]**  \n",
    "* Language Model RNN(순환신경망) 개념을 이용해서 Lyricist AI 를 만들어보자 \n",
    "\n",
    "* Lyricist AI는 노래의 가사를 학습해서 스스로 노래 가사 문장을 생성해내는 언어 모델 인공지능이다.\n",
    "  \n",
    "\n",
    "* 언어모델은 n-1개의 단어 시퀀스가 주어졌을 때, $n$번째 단어 $w_n$으로 무엇이 올지를 예측하는 확률모델이다.\n",
    "\n",
    "* 모델에는 1개의 Embedding Layer, 2개의 LSTM Layer, 1개의 Dense Layer 로 구성된다.\n",
    "\n",
    "**[dataset]**\n",
    "* 영문 노래 가사가 적혀져 있는 49개의 파일을 사용함    \n",
    "* 총 187088 개의 문장이 포함됨  \n",
    "* 토큰의 개수가 15개를 넘어가는 문장은 학습 데이터에서 제외했음  \n",
    "* 데이터 전처리 후 총 데이터의 20%를 평가 데이터셋으로 사용함  \n",
    "\n",
    "**[Prepartion]**  \n",
    "\n",
    "* dataset으로 사용할 파일들을 업로드하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intensive-problem",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. 데이터 읽어오기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustained-landing",
   "metadata": {},
   "source": [
    "* glob.glob() : 유닉스 스타일 경로명 패턴 확장 -> lyrics 폴더 안의 약 49개 파일의 경로이름 리스트 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "reserved-court",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_file_path에 속한 총 파일의 개수는 49 입니다.\n",
      "\n",
      "데이터 크기: 187088\n",
      "\n",
      "Examples:\n",
      " ['At first I was afraid', 'I was petrified', 'I kept thinking I could never live without you']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "print(f'txt_file_path에 속한 총 파일의 개수는 {len(txt_list)} 입니다.\\n')\n",
    "raw_corpus = []\n",
    "\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"\\nExamples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "material-amsterdam",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At first I was afraid\n",
      "I was petrified\n",
      "I kept thinking I could never live without you\n",
      "By my side But then I spent so many nights\n",
      "Just thinking how you've done me wrong\n",
      "I grew strong\n",
      "I learned how to get along And so you're back\n",
      "From outer space\n",
      "I just walked in to find you\n",
      "Here without that look upon your face I should have changed that fucking lock\n",
      "I would have made you leave your key\n",
      "If I had known for just one second\n",
      "You'd be back to bother me Well now go,\n",
      "Walk out the door\n",
      "Just turn around\n",
      "Now, you're not welcome anymore Weren't you the one\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue   #- 길이가 0인 문장 건너뛰기\n",
    "    if sentence[-1] == \":\": continue  #- 문장의 끝이 : 인 문장 건너뛰기\n",
    "\n",
    "    if idx > 15: break   #- 우선 문장 15개 확인\n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metric-afternoon",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. 데이터 전처리 | Data Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affiliated-assurance",
   "metadata": {},
   "source": [
    "### 2-1. 문장 정제하기\n",
    "* 정규식(Regex)를 이용해 데이터를 정제한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "imperial-return",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> at first i was afraid <end>',\n",
       " '<start> i was petrified <end>',\n",
       " '<start> i kept thinking i could never live without you <end>',\n",
       " '<start> by my side but then i spent so many nights <end>',\n",
       " '<start> just thinking how you ve done me wrong <end>',\n",
       " '<start> i grew strong <end>',\n",
       " '<start> i learned how to get along and so you re back <end>',\n",
       " '<start> from outer space <end>',\n",
       " '<start> i just walked in to find you <end>',\n",
       " '<start> here without that look upon your face i should have changed that fucking lock <end>']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def preprocess_sentence(sentence): #- 정규표현식을 이용한 필터링 함수\n",
    "    sentence = sentence.lower().strip() #- 소문자로 바꾸고, 양쪽 공백 지우기\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) #- 특수문자의 양쪽에 공백을 넣기\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) #- 여러개의 공백은 하나의 공백으로 바꾸기\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) #- a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꾸기\n",
    "    sentence = sentence.strip() #- 양쪽 공백 지우기 \n",
    "    sentence = '<start> ' + sentence + ' <end>' #- 문장의 시작에는 <start>, 끝에는 <end> 추가\n",
    "    return sentence\n",
    "\n",
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus: #- raw_corpus 는 문장 리스트? \n",
    "    if len(sentence) == 0 : continue #- 길이가 0인 문장은 건너뜀 \n",
    "    \n",
    "    preprocessed_sentence = preprocess_sentence(sentence) \n",
    "    corpus.append(preprocessed_sentence) #- corpus 리스트에는 필터링된 문장들이 포함됨\n",
    "\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impaired-footwear",
   "metadata": {},
   "source": [
    "### 2-2. 데이터의 토큰화\n",
    "* 텐서플로우는 자연어 처리를 위한 여러가지 모듈을 제공함.\n",
    "* tf.keras.preprocessing.text.Tokenizer 패키지는 정제된 데이터를 토큰화하고, 단어사전을 만들어주며, 데이터를 숫자로 변환해줌\n",
    "* 이처럼 데이터를 숫자로 변환하는 것을 벡터화(vetorize)라고 하며, 숫자로 변환된 데이터를 텐서(tensor) 라고 칭함\n",
    "* 텐서플로우로 만든 모델의 입출력 데이터는 실제로 이러한 **텐서**로 변환되어 처리되는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "continued-serbia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정규식을 이용한 필터링 후의 문장의 개수는 총 175986개 입니다.\n",
      "이후 token의 개수가 15개를 초과하는 문장을 배제하여, 우리가 사용할 문장의 개수는 총 156227개 입니다\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=15000,  #- num_words 를 단어 빈도수가 높은 15000개만 사용함\n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    \n",
    "    list_corpus = tokenizer.fit_on_texts(corpus) #- 문자 데이터를 입력받아서 리스트의 형태로 변환 \n",
    "    tensor = tokenizer.texts_to_sequences(corpus) #- 텍스트 안의 단어들을 숫자의 시퀀스 형태로 변환\n",
    "    print(f'정규식을 이용한 필터링 후의 문장의 개수는 총 {len(tensor)}개 입니다.')\n",
    "    \n",
    "    a = []\n",
    "    for i in tensor:\n",
    "        if len(i) <= 15:\n",
    "            a.append(i)\n",
    "        tensor = a\n",
    "    print(f'이후 token의 개수가 15개를 초과하는 문장을 배제하여, 우리가 사용할 문장의 개수는 총 {len(tensor)}개 입니다')\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post') #- 숫자 0을 이용해 같은 길이의 시퀀스로 변환 \n",
    "    \n",
    "    return tensor, tokenizer\n",
    "\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-numbers",
   "metadata": {},
   "source": [
    "### 2-3. 텐서를 소스와 타겟으로 분리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elegant-lesson",
   "metadata": {},
   "source": [
    "* **소스문장(Source Sentence)** : 자연어처리 분야에서 모델의 입력이 되는 문장\n",
    "* **타겟 문장(Target Sentence)** : 정답 역할을 하게 될 모델의 출력 문장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "nonprofit-alarm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : i\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n",
      "[  2  71 241   5  57 665   3   0   0   0   0   0   0   0]\n",
      "[ 71 241   5  57 665   3   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word: #- tokenizer.index_word를 이용하여 각 토큰의 인덱스를 확인\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break\n",
    "\n",
    "src_input = tensor[:, :-1] #- tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성  \n",
    "tgt_input = tensor[:, 1:] #- tensor에서 <start>를 잘라내서 타겟 문장을 생성\n",
    "\n",
    "print(src_input[0]) #- 소스는 2(<start>)에서 시작해서 3(<end>)으로 끝난 후 0(<pad>)로 채워짐\n",
    "print(tgt_input[0]) #- 타겟은 2로 시작하지 않고 소스를 왼쪽으로 한 칸 시프트 한 형태를 가지고 있음 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upset-armenia",
   "metadata": {},
   "source": [
    "### 2-4. 데이터셋 객체 생성하기\n",
    "* 텐서플로우는 텐서로 생성된 데이터를 이용해 tf.data.Dataset 객체를 생성하는 방법을 흔히 사용함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "major-immigration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input)) #- tf.data.Dataset객체를 생성\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plastic-postage",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. 인공지능 학습시키기\n",
    "* tf.keras.Model을 Subclassing하는 방식을 선택\n",
    "* 모델에는 1개의 Embedding 레이어, 2개의 LSTM 레이어, 1개의 Dense 레이어로 구성됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "missing-portugal",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input,tgt_input,test_size=0.2, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bigger-survey",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (124981, 14)\n",
      "Target Train: (124981, 14)\n"
     ]
    }
   ],
   "source": [
    "print(\"Source Train:\", np.shape(enc_train))\n",
    "print(\"Target Train:\", np.shape(dec_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acquired-minority",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documented-metabolism",
   "metadata": {},
   "source": [
    "* Embedding Layer의 역할  \n",
    "    * 입력 tensor에 들어있는 단어사전의 인덱스 값을 해당 인덱스 번째의 워드 벡터로 바꿔줌\n",
    "    * 워드 벡터는 의미 벡터 공간에서 단어의 추상적 표현으로 사용됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "resident-default",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "expensive-liberty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "610/610 [==============================] - 220s 352ms/step - loss: 3.9733\n",
      "Epoch 2/10\n",
      "610/610 [==============================] - 218s 357ms/step - loss: 3.0213\n",
      "Epoch 3/10\n",
      "610/610 [==============================] - 220s 360ms/step - loss: 2.8398\n",
      "Epoch 4/10\n",
      "610/610 [==============================] - 221s 361ms/step - loss: 2.7112\n",
      "Epoch 5/10\n",
      "610/610 [==============================] - 220s 360ms/step - loss: 2.6016\n",
      "Epoch 6/10\n",
      "610/610 [==============================] - 218s 358ms/step - loss: 2.5008\n",
      "Epoch 7/10\n",
      "610/610 [==============================] - 218s 357ms/step - loss: 2.4064\n",
      "Epoch 8/10\n",
      "610/610 [==============================] - 217s 356ms/step - loss: 2.3206\n",
      "Epoch 9/10\n",
      "610/610 [==============================] - 218s 356ms/step - loss: 2.2416\n",
      "Epoch 10/10\n",
      "610/610 [==============================] - 217s 356ms/step - loss: 2.1663\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcf360b3950>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "advance-filling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  3840256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  15376025  \n",
      "=================================================================\n",
      "Total params: 32,855,961\n",
      "Trainable params: 32,855,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clinical-stable",
   "metadata": {},
   "source": [
    "## 4. 학습모델 평가하기\n",
    "* 모델이 작문을 잘하는지 평가하는 가장 확실한 방법은 작문을 시켜보고 직접 평가하는 것임  \n",
    "* generate_text 함수는 모델에게 시작 문장을 전달하면 모델이 시작 문장을 바탕으로 작문을 진행하게 함  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opponent-cornwall",
   "metadata": {},
   "source": [
    "### 4-1. Test Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "arabic-steam",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123/123 - 16s - loss: 2.0865\n",
      "test loss: 2.086527109146118\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(enc_val, dec_val, verbose=2, batch_size=256)\n",
    "\n",
    "print('test loss:', results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "under-associate",
   "metadata": {},
   "source": [
    "### 4-2. 작문 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "small-rates",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환함\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    \n",
    "    while True:\n",
    "        # 1\n",
    "        predict = model(test_tensor) \n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3 \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-valley",
   "metadata": {},
   "source": [
    "**[Lyricist AI의 동작]** : 단어를 하나씩 예측해 문장을 만듬  \n",
    "1. 입력받은 문장의 텐서를 입력함 \n",
    "2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냄  \n",
    "3. 위에서 예측된 word index를 문장 뒤에 붙임  \n",
    "4. 모델이 \\<end\\> 를 예측했거나, max_len에 도달했다면 문장 생성을 마침  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "contemporary-knife",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you , i love you <end> '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "primary-elder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> yeh i t wanna be with you <end> '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> yeh\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "developing-timber",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> girl i m a voodoo chile <end> '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> girl\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "quality-benefit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> sky is the limit and you know that you can have <end> '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> sky\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "numerous-atmosphere",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> you know i m a motherfucking monster <end> '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> you\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "virgin-timothy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> this is the only way <end> '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> this\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "arctic-canberra",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> they don t know what to do <end> '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> they\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "running-oxide",
   "metadata": {},
   "source": [
    "### 4-3. 최종평가\n",
    "**test_loss : 2.086527109146118  \n",
    "작문실력 : 대체로 자연스럽게 문장을 잘 만들어냈음.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hairy-twins",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. 자가평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clear-philadelphia",
   "metadata": {},
   "source": [
    "언어모델을 처음 만들어보는데, 생각보다 문장을 너무 잘 생성해서 놀랐다.    \n",
    "자연어처리는 CV와 학습 흐름이 많이 달랐는데, 토큰화된 데이터를 다층의 순환신경망에 넣어서 학습을 시켰다.   \n",
    "이번에 사용한 RNN 은 요즘엔 잘 안쓰이는 모델인 것 같은데도 (더욱 성능 좋은 모델이 많기 때문에)  \n",
    "성능이 만족스럽게 나왔다. 앞으로 더욱 발전된 형태의 모델을 사용하면 어떤 결과가 나올지 기대된다.     \n",
    "  \n",
    "이번 노드를 공부하면서 조원들과 \"인공지능 가짜뉴스의 영향력\"에 대한 토론을 했다.   \n",
    "단순히 인간처럼 자연스럽게 언어를 사용하는 인공지능을 만드는 것을 목표로 하는 것이 아니라,  \n",
    "이를 어떻게 활용할 것인지, 어떤 이로움이 있는지에 대한 고민이 필요하다고 느낀다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "mobile-destination",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>평가사항</th>\n",
       "      <th>세부평가사항</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.</th>\n",
       "      <td>가사 텍스트 생성 모델이 정상적으로 동작하는가?</td>\n",
       "      <td>텍스트 제너레이션 결과가 그럴듯한 문장이 생성되는가?</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.</th>\n",
       "      <td>데이터 전처리와 데이터셋 구성 과정이 체계적으로 진행되었나?</td>\n",
       "      <td>특수문자 제거,토크나이저 생성, 패딩처리 등의 과정을 빠짐없이 수행했는가?</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.</th>\n",
       "      <td>텍스트 생성모델이 안정적으로 학습되었나?</td>\n",
       "      <td>텍스트 생성모델의 validation loss가 2.2 이하로 낮아졌는가?</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 평가사항  \\\n",
       "1.         가사 텍스트 생성 모델이 정상적으로 동작하는가?   \n",
       "2.  데이터 전처리와 데이터셋 구성 과정이 체계적으로 진행되었나?   \n",
       "3.             텍스트 생성모델이 안정적으로 학습되었나?   \n",
       "\n",
       "                                       세부평가사항 answer  \n",
       "1.              텍스트 제너레이션 결과가 그럴듯한 문장이 생성되는가?    Yes  \n",
       "2.  특수문자 제거,토크나이저 생성, 패딩처리 등의 과정을 빠짐없이 수행했는가?    Yes  \n",
       "3.  텍스트 생성모델의 validation loss가 2.2 이하로 낮아졌는가?    Yes  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#- pandas 연습해보기\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "data = {'평가사항': ['가사 텍스트 생성 모델이 정상적으로 동작하는가?','데이터 전처리와 데이터셋 구성 과정이 체계적으로 진행되었나?', '텍스트 생성모델이 안정적으로 학습되었나?'], '세부평가사항':['텍스트 제너레이션 결과가 그럴듯한 문장이 생성되는가?','특수문자 제거,토크나이저 생성, 패딩처리 등의 과정을 빠짐없이 수행했는가?','텍스트 생성모델의 validation loss가 2.2 이하로 낮아졌는가?'], 'answer': ['Yes', 'Yes', 'Yes']}\n",
    "check = pd.DataFrame(data, columns = ['평가사항','세부평가사항','answer'], index = ['1.','2.','3.'])\n",
    "check"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
